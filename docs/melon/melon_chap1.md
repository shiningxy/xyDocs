# 西瓜书第一章 & 南瓜书第一章

> 第二章 : [西瓜书第二章 & 南瓜书第二章](melon_chap2.md)

## 基本术语

> 奥卡姆剃刀原则：如无必要，勿增实体，即简单有效原理。拉丁文原文：
> > Numquam ponenda est pluralitas sine necessitate.（避重趋轻）
> >
> > Pluralitas non est ponenda sine necessitate.（避繁逐简）
> >
> > Frustra fit per plura quod potest fieri per pauciora.（以简御繁）
> >
> > Entia non sunt multiplicanda praeter necessitatem.（避虚就实）
* 样本：是关于一个事件或对象的描述。
* 样本空间：也称输入空间或属性空间。由于样本采用的是表明各个特征取值的特征向量来进行表示，有向量便会有向量所在的空间，因此称表示样本的特征向量所在的空间为样本空间。通常用花式大写的$ \mathcal{x} $表示。
* 数据集：令集合$D={x_1,x_2,...,x_m}$表示包含$m$个样本的数据集，一般同一份数据集中的每个样本包含相同个数的特征，第$i$个样本的$d$维特征向量记为$ x_i=(x_{i1},x_{i2},...,x_{id}) $，其中$ d $为样本的特征个数，$ x_{ij} $为第$i$个样本的第$j$个特征的取值。
* 模型：划分数据集为训练集和测试集后，使用机器学习算法，让其在训练集上进行学习(训练)，产出得到的就是模型。表示我们已经默认样本的背后是存在某种潜在的规律，这种潜在的规律为真相或真实。

> 最后用测试集来测试模型效果。同一个机器学习算法，使用不同的参数配置或不同的训练集，训练得到的模型通常都不同。

* 标记：机器学习的本质就是在学习样本在某个方面的表现是否存在潜在的规律，称该方面的信息为标记。一般第$i$个样本的标记的数学表示为$y_i$，标记所在的空间称为标记空间或输出空间，数学表示为花式大写的$ \mathcal{Y} $
* 分类：预测离散值，返回类别
* 回归：预测连续值，返回数值
* 二分类：只涉及两个类别的分类任务，通常称其中一个类为正类，另一个类为反类
* 多分类：涉及多个类别的分类任务
* 测试：训练完模型后，使用其进行预测的过程
* 测试样本：被预测的样本
* 聚类：将样本分成若干个簇，簇内样本相似，簇间样本不相似

> 学习任务可以大致划分为两大类：监督学习和无监督学习，分类和回归是监督学习的代表，聚类是无监督学习的代表。

* 泛化能力：模型对新样本的预测能力
* 独立同分布：样本之间相互独立，且来自同一分布

> 一般而言，训练样本越多，得到的信息越多，训练所得的模型的泛化能力越强。

## 假设空间

* 科学推理：从已知的事实推导出未知的事实
* 科学推理两大基本手段：归纳和演绎
* 归纳：从特殊到一般，从个别到整体
* 演绎：从一般到特殊，从整体到个别

> 可以把学习过程看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配的假设。

* 假设空间：所有可能假设的集合
* 版本空间：学习过程是基于有限样本训练集进行的，可能有多个假设与训练集一致，即存在着一个与训练集一致的假设集合，称之为版本空间。

## 归纳偏好

> 归纳偏好：通过学习得到的模型对应了假设空间中的一个假设。机器学习算法在学习过程中对某种类型假设的偏好，称为归纳偏好。

> 任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上等效的假设所迷惑，而无法产生确定的学习结果。若模型时而给出一个答案，时而给出另一个答案，那么这个模型的归纳偏好就是不确定的，这样的学习结果显然是没意义的。

> 注意结合南瓜书推导公式1-1和1-2

**结合书中式1-2可以看出，对于二分类问题，总误差与学习算法无关**，无论算法多聪明多笨拙，它们的期望性能式相同的，这就是没有免费的午餐定理（NFL定理）

NFL定理的简单论述过程中假设了希望学习的真实目标函数的均匀分布，而实际问题中并非如此。因此，NFL定理并不是说学习算法的期望性能是不可能超过某个基准的，而是说，对于任何学习算法，其期望性能都不可能超过某个基准，而这个基准是由学习问题的特性决定的。

**脱离了具体问题，空谈算法毫无意义**，必须针对具体的学习问题来谈论算法的优劣。学习算法自身的归纳偏好与问题是否相匹配，往往会起到决定性作用。

# 习题

> 解答中，后续更新

### 1. Question: 表1.1中若只包含编号为1和4的两个样例，尝试给出相应的版本空间

Answer:
> 依照自顶向下、从一般到特殊，或是自底向下、从特殊到一般的搜索策略对假设空间进行搜索，搜索过程中不断删除与正例不一致的假设，和（或）与反例一致的假设。最终将会获得与训练集一致（即对所有训练样本都能狗正确判断）的假设
>
> 在此采用自顶向下、从一般到特殊的策略对假设空间进行搜索。
>
> 1. 首先，根据表1.1可以计算假设空间规模大小为$3 \times 4 \times 4 + 1 = 49$，若只有编号1和4两个样例，则将$好瓜=(色泽青绿)\wedge(根蒂蜷缩)\wedge(敲声浊响)$作为正例，$坏瓜=(色泽乌黑)\wedge(根底稍蜷)\wedge(敲声沉闷)$作为反例。 
>
> 2. 找到与正例不一致的假设（推测不为好瓜的假设），这包括了多种假设，如色泽为乌黑的，或根蒂为硬挺、稍蜷的，或敲声为清脆、沉闷的，含有这些特征的假设，都可以剔除。特别注意，若色泽、根蒂、敲声的中某些个属性的取值尚未确定，但存在值已确定的属性与正例一致的假设，不应该剔除。
>    * 例如：
>       * $(色泽=*)\wedge(根蒂=*)\wedge(敲声=*)$ 保留
>       * $(色泽=青绿)\wedge(根蒂=*)\wedge(敲声=*)$  保留
>       * $(色泽=乌黑)\wedge(根蒂=*)\wedge(敲声=*)$  剔除
>
> 3. 接下来找到与反例一致的假设（推测为坏瓜的假设），这包括了多种假设，如色泽为乌黑的，或根蒂为稍蜷的，敲声为沉闷的，含有这些特征的假设，都可以剔除。特别注意，若色泽、根蒂、敲声的中某些个属性的取值尚未确定，但存在值已确定的属性与反例一致的假设，应该剔除。
>    * 例如：
>       * $(色泽=*)\wedge(根蒂=*)\wedge(敲声=*)$ 剔除
>       * $(色泽=青绿)\wedge(根蒂=*)\wedge(敲声=*)$  保留
>       * $(色泽=乌黑)\wedge(根蒂=*)\wedge(敲声=*)$  剔除
> 
> 4. 第2步和第3步中，存在部分重叠的假设，最终在所有假设中完成第2步和第3步后，共保留了7个假设，则版本空间内有7个假设，分别是：
>   * 色泽=青绿，根蒂=\*，敲声=\*
>   * 色泽=\*，根蒂=蜷缩，敲声=\*
>   * 色泽=\*，根蒂=\*，敲声=浊响
>   * 色泽=青绿，根蒂=蜷缩，敲声=\*
>   * 色泽=\*，根蒂=蜷缩，敲声=浊响
>   * 色泽=青绿，根蒂=\*，敲声=浊响
>   * 色泽=青绿，根蒂=蜷缩，敲声=浊响


2. Question: 与使用单个合取式来进行假设表示相比，使用“祈合范式”将使得假设空间具有更强的表示能力。例如：
    $$好瓜 \longleftrightarrow ((色泽=*) \wedge (根蒂=蜷缩) \wedge (敲声=*)) \vee ((色泽=乌黑) \wedge (根蒂=*) \wedge (敲声=沉闷))$$
会把 $ (色泽=青绿) \wedge (根蒂=蜷缩) \wedge (敲声=清脆)$ 以及 $ (色泽=乌黑) \wedge (根蒂=硬挺) \wedge (敲声=沉闷)$ 都分类为好瓜。若使用最多包含 $k$ 个合取式的祈合范式来表达表1.1西瓜分类问题的假设空间，试估算共有多少种可能的假设。




3. Question: 若数据包含噪声，则假设空间中有可能不存在与所有训练样本都一致的假设。在此情形下，试涉及一种归纳偏好用于假设选择。





4. Question: 本章1.4节在论述没有免费的午餐定理时，默认使用了分类错误率作为性能度量来对分类器进行评估，若换用其他性能度量 $ \ell $，则书中式1.1将改为
    $$E_{ote}(\mathfrak{L}_a|X,f)=\sum_{h} \sum_{x\in \mathcal{X}-\mathcal{X}} P(x) \ell (h(x), f(x))P(h|X,\mathcal{a})$$
试证明没有免费的午餐定理仍成立




5. Question: 试述机器学习能在互联网搜索的哪些环节起什么作用。


