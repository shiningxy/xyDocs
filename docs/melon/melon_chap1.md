# 西瓜书第一章 & 南瓜书第一章

> 第二章 : [西瓜书第二章 & 南瓜书第二章](melon_chap2.md)

## 基本术语

> 奥卡姆剃刀原则：如无必要，勿增实体，即简单有效原理。拉丁文原文：
> > Numquam ponenda est pluralitas sine necessitate.（避重趋轻）
> >
> > Pluralitas non est ponenda sine necessitate.（避繁逐简）
> >
> > Frustra fit per plura quod potest fieri per pauciora.（以简御繁）
> >
> > Entia non sunt multiplicanda praeter necessitatem.（避虚就实）
* 样本：是关于一个事件或对象的描述。
* 样本空间：也称输入空间或属性空间。由于样本采用的是表明各个特征取值的特征向量来进行表示，有向量便会有向量所在的空间，因此称表示样本的特征向量所在的空间为样本空间。通常用花式大写的$ \mathcal{x} $表示。
* 数据集：令集合$D={x_1,x_2,...,x_m}$表示包含$m$个样本的数据集，一般同一份数据集中的每个样本包含相同个数的特征，第$i$个样本的$d$维特征向量记为$ x_i=(x_{i1},x_{i2},...,x_{id}) $，其中$ d $为样本的特征个数，$ x_{ij} $为第$i$个样本的第$j$个特征的取值。
* 模型：划分数据集为训练集和测试集后，使用机器学习算法，让其在训练集上进行学习(训练)，产出得到的就是模型。表示我们已经默认样本的背后是存在某种潜在的规律，这种潜在的规律为真相或真实。

> 最后用测试集来测试模型效果。同一个机器学习算法，使用不同的参数配置或不同的训练集，训练得到的模型通常都不同。

* 标记：机器学习的本质就是在学习样本在某个方面的表现是否存在潜在的规律，称该方面的信息为标记。一般第$i$个样本的标记的数学表示为$y_i$，标记所在的空间称为标记空间或输出空间，数学表示为花式大写的$ \mathcal{Y} $
* 分类：预测离散值，返回类别
* 回归：预测连续值，返回数值
* 二分类：只涉及两个类别的分类任务，通常称其中一个类为正类，另一个类为反类
* 多分类：涉及多个类别的分类任务
* 测试：训练完模型后，使用其进行预测的过程
* 测试样本：被预测的样本
* 聚类：将样本分成若干个簇，簇内样本相似，簇间样本不相似

> 学习任务可以大致划分为两大类：监督学习和无监督学习，分类和回归是监督学习的代表，聚类是无监督学习的代表。

* 泛化能力：模型对新样本的预测能力
* 独立同分布：样本之间相互独立，且来自同一分布

> 一般而言，训练样本越多，得到的信息越多，训练所得的模型的泛化能力越强。

## 假设空间

* 科学推理：从已知的事实推导出未知的事实
* 科学推理两大基本手段：归纳和演绎
* 归纳：从特殊到一般，从个别到整体
* 演绎：从一般到特殊，从整体到个别

> 可以把学习过程看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配的假设。

* 假设空间：所有可能假设的集合
* 版本空间：学习过程是基于有限样本训练集进行的，可能有多个假设与训练集一致，即存在着一个与训练集一致的假设集合，称之为版本空间。

## 归纳偏好

> 归纳偏好：通过学习得到的模型对应了假设空间中的一个假设。机器学习算法在学习过程中对某种类型假设的偏好，称为归纳偏好。

> 任何一个有效的机器学习算法必有其归纳偏好，否则它将被假设空间中看似在训练集上等效的假设所迷惑，而无法产生确定的学习结果。若模型时而给出一个答案，时而给出另一个答案，那么这个模型的归纳偏好就是不确定的，这样的学习结果显然是没意义的。

> 注意结合南瓜书推导公式1-1和1-2

**结合书中式1-2可以看出，对于二分类问题，总误差与学习算法无关**，无论算法多聪明多笨拙，它们的期望性能式相同的，这就是没有免费的午餐定理（NFL定理）

NFL定理的简单论述过程中假设了希望学习的真实目标函数的均匀分布，而实际问题中并非如此。因此，NFL定理并不是说学习算法的期望性能是不可能超过某个基准的，而是说，对于任何学习算法，其期望性能都不可能超过某个基准，而这个基准是由学习问题的特性决定的。

**脱离了具体问题，空谈算法毫无意义**，必须针对具体的学习问题来谈论算法的优劣。学习算法自身的归纳偏好与问题是否相匹配，往往会起到决定性作用。

# 习题

1. Question: 表1.1中若只包含编号为1和4的两个样例，尝试给出相应的版本空间

> Answer:
> 依照自顶向下、从一般到特殊，或是自底向下、从特殊到一般的搜索策略对假设空间进行搜索，搜索过程中不断删除与正例不一致的假设，和（或）与反例一致的假设。最终将会获得与训练集一致（即对所有训练样本都能狗正确判断）的假设
>
> 在此采用自顶向下、从一般到特殊的策略对假设空间进行搜索。
>
> 1. 首先，根据表1.1可以计算假设空间规模大小为$3 \times 4 \times 4 + 1 = 49$，若只有编号1和4两个样例，则将$好瓜=(色泽青绿)\wedge(根蒂蜷缩)\wedge(敲声浊响)$作为正例，$坏瓜=(色泽乌黑)\wedge(根底稍蜷)\wedge(敲声沉闷)$作为反例。 
>
> 2. 找到与正例不一致的假设（推测不为好瓜的假设），这包括了多种假设，如色泽为乌黑的，或根蒂为硬挺、稍蜷的，或敲声为清脆、沉闷的，含有这些特征的假设，都可以剔除。特别注意，若色泽、根蒂、敲声的中某些个属性的取值尚未确定，但存在值已确定的属性与正例一致的假设，不应该剔除。
>    * 例如：
>       * $(色泽=*)\wedge(根蒂=*)\wedge(敲声=*)$ 保留
>       * $(色泽=青绿)\wedge(根蒂=*)\wedge(敲声=*)$  保留
>       * $(色泽=乌黑)\wedge(根蒂=*)\wedge(敲声=*)$  剔除
>
> 3. 接下来找到与反例一致的假设（推测为坏瓜的假设），这包括了多种假设，如色泽为乌黑的，或根蒂为稍蜷的，敲声为沉闷的，含有这些特征的假设，都可以剔除。特别注意，若色泽、根蒂、敲声的中某些个属性的取值尚未确定，但存在值已确定的属性与反例一致的假设，应该剔除。
>    * 例如：
>       * $(色泽=*)\wedge(根蒂=*)\wedge(敲声=*)$ 剔除
>       * $(色泽=青绿)\wedge(根蒂=*)\wedge(敲声=*)$  保留
>       * $(色泽=乌黑)\wedge(根蒂=*)\wedge(敲声=*)$  剔除
> 
> 4. 第2步和第3步中，存在部分重叠的假设，最终在所有假设中完成第2步和第3步后，共保留了7个假设，则版本空间内有7个假设，分别是：
>   * 色泽=青绿，根蒂=\*，敲声=\*
>   * 色泽=\*，根蒂=蜷缩，敲声=\*
>   * 色泽=\*，根蒂=\*，敲声=浊响
>   * 色泽=青绿，根蒂=蜷缩，敲声=\*
>   * 色泽=\*，根蒂=蜷缩，敲声=浊响
>   * 色泽=青绿，根蒂=\*，敲声=浊响
>   * 色泽=青绿，根蒂=蜷缩，敲声=浊响

---

2. Question: 与使用单个合取式来进行假设表示相比，使用“祈合范式”将使得假设空间具有更强的表示能力。例如：
    $$好瓜 \longleftrightarrow ((色泽=*) \wedge (根蒂=蜷缩) \wedge (敲声=*)) \vee ((色泽=乌黑) \wedge (根蒂=*) \wedge (敲声=沉闷))$$
会把 $ (色泽=青绿) \wedge (根蒂=蜷缩) \wedge (敲声=清脆)$ 以及 $ (色泽=乌黑) \wedge (根蒂=硬挺) \wedge (敲声=沉闷)$ 都分类为好瓜。若使用最多包含 $k$ 个合取式的祈合范式来表达表1.1西瓜分类问题的假设空间，试估算共有多少种可能的假设。

> 祈取是最常用的逻辑联结词，表示“或”的意思，使用$\vee$来表示；合取表示“且”的意思，使用$\wedge$来表示
> 
> 单个合取式是使用$\wedge$将两个或两个以上的命题联结起来的命题形式
> 
> 单个祈取式是使用$\vee$将两个或两个以上的命题联结起来的命题形式
> 
> 祈合范式是若干个简单合取式的祈取式
>

> Answer: 对于每个合取式，它可以从3个属性中选择任意一个或选择不包含该属性，因此每个合取式有 $2^3=8$ 种可能的形式。对于最多包含 $k$ 个合取式的祈合范式，每个位置可以选择任意一个合取式或选择不包含合取式，因此共有 $9^k$ 种可能的形式。
> 
> 需要注意的是，这里的祈合范式中包含的合取式数量最多为 $k$，而不是恰好为 $k$，因此需要将每个数量小于等于 $k$ 的祈合范式都计算在内。因此，假设空间的大小为：
> 
> $$\sum_{i=0}^k 9^i = \frac{9^{k+1}-1}{8}$$
> 
> 当 $k=1$ 时，假设空间大小为 $5$，即可以用 $5$ 种不同的祈合范式表示所有可能的假设；当 $k=2$ 时，假设空间大小为 $46$；当 $k=3$ 时，假设空间大小为 $421$；当 $k=4$ 时，假设空间大小为 $3794$，以此类推。可以看到，随着 $k$ 的增加，假设空间的大小呈指数级增长。

---

3. Question: 若数据包含噪声，则假设空间中有可能不存在与所有训练样本都一致的假设。在此情形下，试设计一种归纳偏好用于假设选择。

> Answer: 当数据包含噪声时，可能存在训练样本之外的数据点或噪声，导致假设空间中不存在与所有训练样本一致的假设。在这种情况下，需要使用归纳偏好来选择最优的假设，即在可能的假设中选择一个最能解释观察到的数据的假设。
> 
> 归纳偏好是指机器学习算法在学习过程中对一些假设进行更喜欢或更倾向的倾向或偏好。通常，归纳偏好有以下几种：
> 
> 简单性偏好：倾向于选择最简单的假设，因为简单的假设更容易被理解和推广，而复杂的假设可能只是过度拟合训练数据。
> 
> 最小错误偏好：倾向于选择能够产生最少错误的假设，即最小化训练数据和测试数据的错误率。
> 
> 最大边缘偏好：倾向于选择具有最大边缘的假设，即能够在正确分类的同时最大化分类边缘的假设。
> 
> 似然偏好：倾向于选择最能解释数据的假设，即使该假设非常复杂，也可能更好地描述数据。
> 
> 在处理包含噪声的数据时，简单性偏好通常是一种合理的选择，因为简单的假设更容易被理解和推广。此外，可以使用交叉验证等技术来检测和减轻过拟合问题，并确保所选假设能够推广到新的数据。
> 
> 需要注意的是，归纳偏好是机器学习算法的一部分，它并不总是适用于所有情况，也可能因任务而异。因此，选择正确的归纳偏好是一项重要的工作，需要考虑任务的性质和数据的特征。

---

4. Question: 本章1.4节在论述没有免费的午餐定理时，默认使用了分类错误率作为性能度量来对分类器进行评估，若换用其他性能度量 $ \ell $，则书中式1.1将改为
    $$E_{ote}(\mathfrak{L}_a|X,f)=\sum_{h} \sum_{x\in \mathcal{X}-\mathcal{X}} P(x) \ell (h(x), f(x))P(h|X,\mathcal{a})$$
试证明没有免费的午餐定理仍成立

> Answer: 假设我们使用性能度量 $\ell$ 对分类器进行评估。我们可以将没有免费的午餐定理重新表述为：“对于任何分类算法 $\mathfrak{L}$，在所有数据分布 $\mathcal{P}$ 上平均的预测性能等于随机猜测”。
>
>换句话说，如果我们考虑所有可能的数据分布，每个分布的概率相等，那么所有分类算法在这些分布上的平均性能相等。这仍然是成立的，即使我们使用其他性能度量 $\ell$。
>
>为了证明这一点，我们可以使用与书中相同的证明方法，其中假设存在一个数据分布 $\mathcal{P}$，对于这个数据分布，有一个分类算法 $\mathfrak{L}$，在这个数据分布上具有比随机猜测更好的性能，即 $P_{\mathcal{P}}(\mathfrak{L}) < \frac{1}{2}$，其中 $P_{\mathcal{P}}(\mathfrak{L})$ 表示在数据分布 $\mathcal{P}$ 上分类算法 $\mathfrak{L}$ 的正确率。
>
>然后，我们可以构造一个新的数据分布 $\mathcal{P}'$，其中所有的样本和标签都与 $\mathcal{P}$ 相同，但是样本的标签随机翻转。也就是说，对于 $\mathcal{P}$ 中的每个样本 $x$，在 $\mathcal{P}'$ 中，它的标签是 $f(x) \oplus 1$，其中 $\oplus$ 表示异或操作。
>
>对于 $\mathfrak{L}$，在数据分布 $\mathcal{P}'$ 上的正确率为 $1 - P_{\mathcal{P}}(\mathfrak{L})$。然而，在 $\mathcal{P}'$ 中，任何其他分类器 $h$ 的正确率也是 $1 - P_{\mathcal{P}}(h)$。因此，对于任何分类器 $h$，在数据分布 $\mathcal{P}'$ 上的平均正确率都是 $1/2$。这意味着没有分类器能在所有数据分布上具有更好的性能，因此没有免费的午餐定理仍然成立。
> 

---

5. Question: 试述机器学习能在互联网搜索的哪些环节起什么作用。

> Answer: 机器学习在互联网搜索的多个环节都起到了重要作用。下面是几个主要的环节：
>
> 搜索结果排序：搜索引擎需要根据查询词对搜索结果进行排序，以使最相关的结果排在最前面。机器学习可以帮助搜索引擎根据多个因素对搜索结果进行排序，例如相关性、权威性、新颖性和可信度等。常见的机器学习方法包括排序学习和回归学习。
>
>查询扩展：查询扩展是指在用户提交查询后，搜索引擎自动扩展查询以提供更多相关的结果。机器学习可以分析用户的查询历史、上下文和其他因素，以确定最佳的查询扩展方法。这通常涉及到自然语言处理、信息检索和深度学习等技术。
>
>意图识别：搜索引擎需要根据用户的查询来确定其意图，以提供最相关的结果。机器学习可以帮助识别查询的意图，并将其映射到最佳的搜索结果。这通常涉及到分类学习、聚类学习和自然语言处理等技术。
>
>广告投放：搜索引擎通常会在搜索结果页面上显示广告，以赚取广告费用。机器学习可以根据用户的搜索历史、上下文和其他因素来预测用户的兴趣，并提供最相关的广告。这通常涉及到推荐系统、分类学习和回归学习等技术。
>
>用户反馈分析：搜索引擎需要根据用户的反馈来优化搜索结果和搜索体验。机器学习可以分析用户的反馈，以确定如何优化搜索结果和搜索体验。这通常涉及到情感分析、分类学习和聚类学习等技术。
>
>总之，机器学习在互联网搜索的多个环节中都扮演着重要的角色，帮助搜索引擎提供更好的搜索结果和搜索体验。
>