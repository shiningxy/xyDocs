# 西瓜书第二章 & 南瓜书第二章


## 经验误差与过拟合

* 错误率：分类错误样本数占样本总数的比例。错误率：$E=a/m$
* 精度：$1-a/m$，即“精度=1-错误率”
* 误差：模型的实际预测输出与样本的真实输出之间的差异
* 训练误差：模型在训练集上的误差，也称经验误差
* 泛化误差：在新样本上的误差

> 在训练集上表现很好的模型，甚至分类错误率为0，精度为100%，这样的模型多数情况下都不好。

* 过拟合：模型在训练集上的误差很小，但在新样本上的误差很大，即模型对训练集的学习过度，导致模型对新样本的预测能力下降。
* 欠拟合：模型在训练集上的误差很大，即模型对训练集的学习不足，导致模型对新样本的预测能力下降。

> 学习能力是否过于强大，是由学习算法和数据内涵共同决定的。

> 机器学习面临的问题通常是NP Hard或更难，而有效的算法必然在多项式时间内运行完成，若可彻底避免过拟合，则通过经验误差最小化就能获得最优解，这就意味着我们构造性地证明了$P=NP$；因此，只要相信了$P \neq NP$，过拟合就不可避免。

* 模型选择：选择何种参数配置，选择何种学习算法

## 评估方法

* 留出法：将数据集$D$划分为互斥的两个集合，其中一个作为训练集，另一个作为测试集，训练集用来训练模型，测试集用来评估模型的性能。
* 交叉验证法：将数据集$D$划分为$k$个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，每次用$k-1$个子集的并集作为训练集，余下的一个子集作为测试集，重复$k$次，最终结果为$k$次测试结果的均值。
* 自助法：每次从数据集中随机有放回地抽取一个样本作为训练集，其余样本作为测试集，重复多次，最终结果为多次测试结果的均值。

* 调参：在模型选择的基础上，对模型的参数进行调整，使得模型在测试集上的性能达到最优。

## 性能度量

> 回归任务最常用的性能度量是均方误差：
> $$ E(f;D)=\frac{1}{m} \sum_{i=1}^{m} (f(x_i)-y_i)^2 $$
> 
> 更一般的，对于数据分布 $ \mathcal{D} $ 和 概率密度函数 $ p(\cdot) $，均方误差可描述为：
> $$ E(f;\mathcal{D}) = \int_{x\sim D}^{} (f(x)-y)^2p(x)dx$$

### 错误率与精度

分类任务中常用**错误率**和**精度**这两种性能度量方式。它们既适用于二分类任务，也适用于多分类任务。

### 查准率、查全率与F1
对于二分类问题：
* TP:真正例
* FP:假正例
* FN:假反例
* TN:真反例
* 查准率：预测为正例的样本中，真实为正例的比例，即$P=\frac{TP}{TP+FP}$
* 查全率：真实为正例的样本中，预测为正例的比例，即$R=\frac{TP}{TP+FN}$
* F1：查准率和查全率的调和平均，即$F_1=\frac{2 \times P \times R}{P + R}=\frac{2TP}{样例总数+TP-TN}$

> 查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；查全率高时，查准率往往偏低。因此，我们需要在查准率和查全率之间进行权衡，这就是$F1$度量的由来。

以查准率为纵轴、查全率作为横轴，得到查准率-查全率曲线，简称P-R曲线

> 若一个模型的P-R曲线被另一个模型的曲线完全“包住”，则可断言后者的性能优于前者。若有交叉则难以断言孰优孰劣。所以人们设计了综合考量查准率、查全率的性能度量。

平衡点(BEP)就是这样一个度量，但它过于简化，更常用$F1$度量

### ROC 和 AUC

ROC曲线根据模型的预测结果对样例进行排序，分别以它们为横纵坐标作图得到ROC曲线。

ROC曲线是以假正例率(FPR)为横轴，真正例率(TPR)为纵轴。

$$TPR=\frac{TP}{TP+FN}$$
$$FPR=\frac{FP}{TN+FP}$$

ROC曲线图中，若一个模型的ROC曲线被另一个模型的曲线完全“包住”，则可断言后者的性能优于前者。若ROC曲线有交叉则难以断言孰优孰劣。

此时如果一定要进行比较，则合理的判断依据是比较ROC曲线下的面积，即AUC。

> AUC可通过对ROC曲线下各部分的面积求和而得。形式化地看，AUC考虑的是样本预测的排序质量，因此它与排序误差有紧密联系。

### 代价敏感错误率与代价曲线

为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”。在非均等代价中，不再简单地使用最小化错误率，而最小化**总体代价**。

# 习题

> 解答中，待更新...
>

1. 数据集包含1000个样本，其中500个正例、500个反例，将其划分为包含70%样本的训练机和30%样本的测试集用于留出法评估，试估算共有多少种划分方式。

> Answer: 训练集应该包含700个样本，测试集应该包含300个样本。
>
>我们需要计算共有多少种将1000个样本分成包含700个样本和300个样本的两个集合的方法。这个问题可以使用组合数学中的组合公式来解决。
>
>组合公式告诉我们，在n个元素中选取k个元素的组合数为：C(n,k) = n! / (k! * (n-k)!)
>
>在这个问题中，n = 1000，k = 700。因此，将1000个样本分成包含700个样本和300个样本的两个集合的方法数为：
>
>$C(1000,700) = 1000! / (700! * (1000-700)!) = 2,983,316,537,583$
>
>也就是说，有大约2.98万亿种将1000个样本分成包含700个样本和300个样本的两个集合的方法。每种划分方式都会产生不同的训练集和测试集，因此留出法评估的结果也会因此而异。


2. 数据集包含100个样本，其中正、反例各一半，假定学习算法所产生的模型是将新样本预测为训练样本数较多的类别（训练样本数相同时进行随机猜测），试给出用10折交叉验证法和留一法分别对错误率进行评估所得的结果。

3. 若学习器A的F1值比学习器B高，试析A的BEP值是否也比B高。

4. 试述真正例率（TPR）、假正例率（FPR）与查准率（P）、查全率（R）之间的联系。

5. 试证明式（2-2）

6. 试述错误率与ROC曲线的联系

7. 试证明一条ROC曲线都有一条代价曲线与之对应，反之亦然

8. Min-max规范化和z-score规范化是两种常用的规范化方法。令$x$和$x'$分别表示变量在规范化前后的取值，相应的，令$x_{min}$和$x_{max}$表示规范化前的最小值和最大值，$x'_{min}$和$x'_{max}$表示规范化后的最小值和最大值，$\bar{x}$和$\sigma_x$分别表示规范化前的均值和标准差，则min-max规范化、z-score规范化分别如下式(2.43)和(2.44)所示。试析二者的优缺点。

$$x' = x'_{min}+ \frac{x-x_{min}}{x_{max}-x_{min}}\times (x'_{max}-x'_{min})$$

$$x'=\frac{x-\bar{x}}{\sigma_x}$$

9. 试述$\chi^2$检验过程

10. 试述在Friedman检验中使用书中式(2.34)与式(2.35)的区别。