# 西瓜书第三章 + 南瓜书第三章

## 线性回归


对离散属性，若属性值间存在“序”关系，可以通过连续化将其转化为连续值，例如三值属性“高度”的取值“高”，“中”，“低”可转化为{1.0,0.5,0.0}。

若属性间不存在序关系，假定有$k$个属性值，通常转化为$k$维向量，例如属性“瓜类”的取值“西瓜”，“南瓜”，“黄瓜”可转化为(0,0,1),(0,1,0),(1,0,0).

若将无序属性连续化，则会不恰当地引入序关系，对后续处理如距离计算等造成误导。

> 离散属性的向量化可以使用sklearn中 one-hot 独热编码等方法来实现。

> 对于离散属性的连续化问题，可以使用pandas中的 as_unordered 和 reorder_categories  来实现有序类别和无序类别的互相转化。先用 s.cat.as_ordered() 将数据列转化为有序类别，再利用 reorder_categories 进行具体的相对大小调整。由此完成了离散属性序的建立，进一步可以实现分类变量的比较。

### 一元线性回归

线性回归试图学得

$$f(x_i) = \omega x_i+b 使得f(x_i)\simeq y_i$$

衡量$f(x)$与$y$之间的差别是确定$\omega$和$b$的关键，均方误差是回归任务中最常用的性能指标。使均方误差最小化：

> 均方误差也称平方损失 (square loss)

$$(\omega^*,b^*) = \argmin_{(\omega,b)} \sum_{i=1}^{m}(f(x_i)-y_i)^2 \\
            = \argmin_{(\omega,b)} \sum_{i=1}^{m}(y_i-\omega x_i - b)^2 $$


> $\omega^*$ 和 $b^*$ 表示 $\omega$ 和 $b$ 的解

它对应了常用的欧几里得距离或简称“欧氏距离”。基于均方误差最小化来进行模型求解的方法称为“最小二乘法”


最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离最小。

求解$\omega$ 和 $b$ ，使 $ E_{(\omega,b)}= \sum_{i=1}^{m} (y_i - \omega x_i - b)^2 $ 最小化的过程，称为线性回归模型的最小二乘“参数估计”(parameter estimation)。我们可将$E_{(\omega,b)}$分别对$\omega$和$b$求导，得到

$$\frac{\partial E_{(\omega,b)}}{\partial \omega} = 2 (\omega \sum_{i=1}^{m} x_i^2 - \sum_{i=1}^{m} (y_i - b)x_i)$$


$$\frac{\partial E_{(\omega,b)}}{\partial \omega} = 2 (mb-\sum_{i=1}^{m}(y_i -\omega x_i))$$

令上两式为零可得到$\omega$ 和 $b$最优解的闭式(closed-form)解

$$\omega = \frac{\sum_{i=1}^{m} y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\frac{1}{m}(\sum_{i=1}^{m} x_i)^2}$$

$$b = \frac{1}{m}\sum_{i=1}^{m}(y_i-\omega x_i)$$

其中，$\bar{x}=\frac{1}{m}\sum_{i=1}^{m}x_i$ 为 $x$的均值

> $E_{\omega,b}$ 是关于$\omega$和$b$的凸函数，当它关于$\omega$和$b$的导数均为0时，得到$\omega$和$b$的最优解


### 多元线性回归

多元线性回归则试图学得$f(\mathbf{x}_i)=\omega^T \mathbf{x}_i+b$使得$f(\mathbf{x}_i \simeq y_i)$，这称为多元线性回归。


线性模型虽然简单，却有丰富的变化。

假设认为输出标记实在指数尺度上变化，那就可以将输出标记的对数作为线性模型逼近的目标，即

$$\ln y=\omega^T \mathbf{x} + b$$

这就是对数线性回归，它实际上时在试图让$e^{\omega^T \mathbf{x}+b}$逼近$y$，上式在形式上仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射，这里的对数函数起到了将线性回归模型的预测值与真实标记联系起来的作用。

![对数线性回归](https://docs-xy.oss-cn-shanghai.aliyuncs.com/%E5%AF%B9%E6%95%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%A4%BA%E6%84%8F%E5%9B%BE.png)


> 考虑单调可微函数$g(\cdot)$令$y=g^{-1}(\omega^T \mathbf{x} + b)$,就得到了广义线性模型，其中函数$g(\cdot)$称为联系函数。

### 对数几率回归

找到单调可微函数将分类任务的真实标记$y$与线性回归模型的预测值联系起来。即可解决分类任务。

二分类任务，最理想的方法是使用单位阶跃函数，若预测值$z$大于0则判为正例，小于0为负例，预测值为临界值零则可以任意判别。如下图像为单位阶跃函数与对数几率函数

![单位阶跃函数](https://docs-xy.oss-cn-shanghai.aliyuncs.com/%E5%8D%95%E4%BD%8D%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0.png)

图中的阶跃函数并不连续，不能直接使用，所以使用对数几率函数作为替代函数：

$$y=\frac{1}{1+e^{-z}}$$

对数几率函数是一种Sigmoid函数，类似的函数还有tanh函数

> pytorch中的函数方法为torch.nn.Sigmoid, torch.nn.Tanh

## 线性判别分析

线性判别分析在二分类问题上最早由Fisher提出，也称“Fisher判别分析”

LDA的思想：给定训练样例集，设法将阳历投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。

![LDA](https://docs-xy.oss-cn-shanghai.aliyuncs.com/LDA.png)

欲使同类阳历的投影点尽可能接近，可以让同类样例投影点的协方差尽可能小，即$\omega^T\sum_{0}\omega + \omega^T\sum_{1}\omega$尽可能小；欲使异类样例的投影点尽可能远离，可以让类中心之间的距离尽可能大，即$\lVert \omega^T \mu_0 - \omega^T \mu_1 \rVert_2^2$尽可能大，同时考虑二者，则得到欲最大化的目标：

$$J=\frac{ \lVert \omega^T \mu_0 - \omega^T \mu_1 \rVert_2^2}{\omega^T\sum_{0}\omega + \omega^T\sum_{1}\omega} \\ 
 = \frac{\omega^T (\mu_0-\mu_1)(\mu_0-\mu_1)^T\omega}{\omega^T(\sum_0+\sum_1) \omega}$$

定义类内散度矩阵(within-class scatter matrix):

$$S_\omega = \sum_{0}+\sum_{1} \\ 
=\sum_{x\in X_0}(\mathbf{x}-\mu_0)(\mathbf{x}-\mu_0)^T+\sum_{x\in X_1}(\mathbf{x}-\mu_1)(x-\mu_1)^T$$

以及类间散度矩阵(between-class scatter matrix):

$$S_b=(\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$$

重写$J$，得到广义瑞利商(generalized Rayleigh quotient)：

$$J=\frac{\omega^T S_b \omega}{\omega^T S_\omega \omega}$$

则LDA最大化的目标是$S_b$与$S_\omega$的广义瑞利商

## 多分类学习

多分类学习的基本思路是**拆解法**，即将多分类任务拆解为多个二分类任务求解。

## 类别不平衡问题

> 当不同类别的训练样例数目有很大差距时，会导致学习样本出现严重问题。

类别不平衡学习的基本策略是**再缩放**：

令$m^+$表示正例数目，$m^-$表示反例数目，则观测几率是$\frac{m^+}{m^-}$，由于我们通常假设训练集是真是样本总体的**无偏采样**，因此观测几率代表了真实几率。

分类器的决策规则是，若$\frac{y}{1-y}>1$，则预测为正例。对规则进行调正，在预测值上添加观测几率倒数。令

$$\frac{y'}{1-y'} = \frac{y}{1-y}\times \frac{m^-}{m^+}$$

通过上式，我们就实现了**再缩放**这一类别不平衡学习的基本策略。

但！“训练集是真实样本总体的无偏采样”这个假设在实际问题中往往不成立。现有技术大体有三类方法，解决这一问题：

1. 欠采样：去除一些反例，使得正反里数目接近
2. 过采样：对数目较少的样例进行过采样，使得其数目增加至与另一类相近，再进行学习
3. 直接基于原始数据集进行学习

> 将再缩放策略嵌入训练好的分类器中，称为阈值移动

> 再缩放也是代价敏感学习的基础

## 习题

> 解答中，待更新

1. 试析在什么情形下式(3.2)不必考虑偏置项$b$

2. 试证明，对于参数$\omega$，对率回归的目标函数(3.18)是非凸的，但其对数似然函数(3.27)是凸的.

3. 编程实现对率回归并给出西瓜数据集$3.0\alpha$上的结果.

4. 选择两个UCI数据集，比较10折交叉验证法和留一法所估计出的对率回归的错误率.

5. 编程实现线性判别分析，并给出西瓜数据集$3.0\alpha$上的结果.

6. 线性判別分析仅在线性可分数据上能获得理想结果，试设计一个改进方法，使其能较好地用于非线性可分数据.

7. 令码长为9,类别数为4,试给出海明距离意义下理论的ECOC二元码并证明之.

8. ECOC编码能起到理想纠错作用的重要条件是：在每一位编码上出错的概率相当且独立.试析多分类任务经ECOC编码后产生的二类分类器满足该条件的可能性及由此产生的影响.

9. 使用OvR和MvM将多分类任务分解为二分类任务求解时，试述为何无需专门针对类別不平衡性进行处理.

10. 试推导出多分类代价敏感学习(仅考虑基于类别的误分类代价)使用“再缩放”能获得理论最优解的条件.
