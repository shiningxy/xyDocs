# 西瓜书第四章 + 南瓜书第四章


## 基本流程

> 决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树，其基本流程遵循简单且直观的“分而治之”策略。

---

决策树的基本算法流程

输入：训练集$D={(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),...,(\mathbf{x}_m,y_m)}$  
&emsp;&emsp;&emsp;属性集$A={a_1,a_2,...,a_d}$

过程：函数TreeGenerate($D$,$A$)

1. 生成结点node;
2. $if$ $D$中样本全属于同一类别$C$  $then$
3. &emsp;将node标记为$C$类叶节点； $return$
4. $end \ if$
5. $if$ $A=\varnothing$ $OR$ $D$中样本在$A$上取值相同 $then$
6. &emsp;将node标记为叶结点，其类别标记为$D$中样本数最多的类； $return$
7. $end \ if$
8. 从$A$中选择最优划分属性$a_*$;
9. $for$ $a_*$的每一个值$a_*$ $do$
10. &emsp;为node生成一个分支；令$D_v$表示$D$中在$a_*$上取值为$a_*^v$的样本子集；
11. &emsp;$if \ D_v$为空 $then$
12. &emsp;&emsp;将分支结点标记为叶结点，其类别标记为$D$中样本最多的类；$return$
13. &emsp;$else$
14. &emsp;&emsp;以TreeGenerate $(D_v,A \backslash \{a_*\})$ 为分支结点
15. &emsp;$end \ if$
16. $end \ for$

输出：以node为根结点的一棵决策树

---

> 显然，决策树生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：
>
> 第一种情形：当前结点包含的样本权属于同一类别，无需划分
>
> 第二种情形：当前属性集为空，或是所有样本在所有属性上的取值相同，无法划分
>
> 第三种情形：当前结点包含的样本集合为空，不能划分
> 
> 第二种情形将当前结点标记为叶结点，并将其类别设定为节点所含样本最多的类别
>
> 第三种情形同样将当前结点标记为叶结点，但将其类别设定为其父结点为其父结点所含样本最多的类别
>
> 这两种情形的处理实质不同，情形二利用当前结点的后验分布，情形三则把父结点的样本分布作为当前结点的先验分布。

## 划分选择

### 信息增益

信息熵：度量样本集合纯度最常用的一种指标

假定当前样本集合$D$中第$k$类样本所占的比例为$p_k(k=1,2,...,|\mathcal{Y}|)$，则$D$的信息熵定义为

$$Ent(D)=-\sum_{k=1}^{\mathcal{Y}}p_k log_{2}p_k$$

$Ent(D)$的值越小，则$D$的纯度越高

第$v$个分支结点包含了$D$中所有在离散属性$a$上取值为$a^v$的样本，记为$D^v$，根据上式计算出$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$|D^v|/|D|$，即样本数越多的分支结点的影响越大，于是可计算出用属性$a$对样本集$D$进行划分所获得的**信息增益**

$$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{D}Ent(D^v)$$

一般而言，信息熵越大，则意味着使用属性$a$来进行划分所获得的**纯度提升**越大。因此，可用信用增益来进行决策树的划分属性选择，即在决策树基本算法第8步中选择属性$a_*=\argmax_{a \in A}Gain(D,a)$。ID3决策树学习算法就是以信息增益为准则来选择划分属性。


### 增益率

$C4.5$决策树算法不直接使用信息增益，而是使用**增益率**来选择最优划分属性。增益率定义为：

$$Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$$

其中

$$IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{D}log_2 \frac{|D^v|}{D}$$

称为属性$a$的**固有值**(intrinsic value)。属性$a$的可能取值数目越多（即$V$越大），则$IV(a)$的值通常越大。

> 增益率准则对可取值数目较少的属性有所偏好，因此，$C4.5$并步直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

### 基尼指数

$CART$决策树使用基尼指数来选择划分属性，数据集$D$的纯度可用基尼值来度量：

$$Gini(D)=\sum_{k=1}^{|\mathcal{Y}|}\sum_{k'\neq k} p_kp_{k'} \\
        = 1 - \sum_{k=1}^{|\mathcal{Y}|}p_{k^2}$$

直观来说，$Gini(D)$反映了数据集$D$中随机抽取两个样本，其类别标记不一致的概率，因此$Gini(D)$越小，则数据集$D$的纯度越高

属性$a$的基尼指数定义为：

$$Gini_index(D,a)=\sum_{v=1}^{V}\frac{D^v}{D}Gini(D^v)$$

## 剪枝处理

决策树剪枝的基本策略有**预剪枝**和**后剪枝**，剪枝是对付决策树“过拟合”的主要手段。

预剪枝：在决策树生成过程中，对每个节点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能特省，则停止划分，将当前结点标记为叶结点。

后剪枝：生成一棵完整的决策树，自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将子树替换为叶结点。

## 连续与缺失值

### 连续值处理

$C4.5$决策树算法中采用的机制是：采用二分法(bi-partition)对连续属性进行处理

### 缺失值处理

要解决两个问题：

1. 如何在属性值缺失的情况下进行划分属性选择？
2. 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

对问题（1），只能根据数据集$D$在属性$a$上没有缺失值的样本子集来判断属性$a$的优劣

对问题（2），可以让同一个样本以不同的概率（样本权值）划入到不同的子结点中去

## 多变量决策树

决策树所形成的分类边界有一个明显的特点：轴平行(axis-parallel)，即它的分类边界由若干个与坐标轴平行的分段组成。分段边界的每一段都是与坐标轴平行的。这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。

多变量决策树能实现如下图所示的**斜划分**，划分边界如灰色（书中红色）线段所示。

![决策树对复杂分类边界的分段近似](https://docs-xy.oss-cn-shanghai.aliyuncs.com/mdt.png)

此类决策树中，非叶结点是一个形如$\sum_{i=1}^{d}\omega_i a_i = t$的线性分类器，其中$\omega_i$是属性$a_i$的权重，$\omega_i$和$t$可在该结点所含的样本集和属性集上学得。多变量决策树不是为每个非叶节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。


