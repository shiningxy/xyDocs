# 西瓜书第五章 + 南瓜书第五章

1943年，[McCulloch and Pitts, 1943]将神经元抽象为简单模型，这就是一直沿用至今的“M-P神经元模型”. 在这个模型中，神经元接收到来自$n$个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，然后通过激活函数处理以产生神经元的输出。

<!-- <img src='https://docs-xy.oss-cn-shanghai.aliyuncs.com/M-P.png' width='50%'> -->

> 对数几率函数是Sigmoid函数的典型代表，把可能在较大范围内变化的输入值挤压到(0,1)输出值范围内，因此有时也称为“挤压函数”(spuashing function)

## 感知机与多层网络

给定训练数据集，权重$\omega_i (i=1,2,...,n)$以及阈值$\theta$可通过学习得到。阈值$\theta$可看作一个固定输入为$-1.0$的哑结点所对应的连接权重为$\omega_{n+1}$，这样，权重和阈值的学习就可统一为权重学习。

对于训练样例$(\mathbf{x}, y)$, 若当前感知机的输出为$\hat{y}$，则感知机的权重将这样调整：

$$\omega_i \leftarrow \omega_i + \Delta\omega_i$$

$$\Delta\omega_i = \eta(y-\hat{y})x_i$$

其中，$\eta\in(0,1)$称为**学习率**。从上式可以看出，如果感知机对训练样例$(\mathbf{x},y)$预测正确，即$\hat{y}=y$，则感知机不发生变化，否则将根据错误的程度进行权重调整。

感知机学习算法的收敛性：感知机学习算法是一种**线性可分**的学习算法，即训练数据集中的正实例点和负实例点在特征空间中是线性可分的。感知机学习算法的收敛性是指，对于线性可分的训练数据集，感知机学习算法能够收敛到一个唯一的解。**感知机甚至不能解决异或这样简单的非线性可分问题。**

要解决非线性可分问题，需要使用多层网络。简单的两层感知机就能解决异或问题，在输出层与输入层之间添加一层神经元，称这一层为隐层或隐含层(hidden layer)，隐含层和输出层神经元都是拥有激活函数的功能神经元。

更一般的，镁层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接。这样的神经网络结构通常称为“多层前馈神经网络”(multi-layer feedforward neural network)。

只需包含隐层，即可称为多层网络。神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接权”以及每个功能神经元的阈值；换言之，神经网络“学”到的东西，蕴含在连接权与阈值中。

## 误差逆传播算法

误差逆传播算法(backpropagation algorithm)是一种用于训练多层前馈神经网络的有效算法。误差逆传播算法的基本思想是，对于给定的训练数据集，首先通过正向计算得到输出层的实际输出，然后通过反向计算输出层和隐藏层之间的误差，最后利用这些误差来调整连接权和阈值。


对训练例$(\mathbf{x}_k, \mathbf{y}_k)$，假定神经网络的输出为$\hat{\mathbf{y}_k}=(\hat{y}_1^k,\hat{y}_2^k,...,\hat{y}_l^k)$，即

$$\hat{y}_j^k=f(\beta_j-\theta_j)$$

则网络在$(\mathbf{x}_k, \mathbf{y}_k)$上的均方误差为

$E_k = \frac{1}{2}\sum_{j=1}^{l}(\hat{y}_{j}^{k} - y_j^k)^2$


BP算法基于梯度下降(gradient descent)策略，以目标的负梯度方向对参数进行调整。对上式的误差$E_k$，给定学习率$\eta$，有

$$\Delta\omega_{hj} = - \eta \frac{\partial E_k}{\partial \omega_{hj}} $$

注意到$\omega_hj$先影响到第$j$个输出层神经元的输入值$\beta_j$，再影响到其输出值$\hat{y}_j^k$，然后影响到$E_k$

学习率$\eta \in (0,1)$控制着算法每一轮迭代中的更新步长，若太大则容易震荡，若太小则收敛速度又会过慢。

BP算法中关于$\omega_{hj}$的更新公式

$$\Delta \omega_{hj} = \eta g_j b_h$$
类似可得

$$\Delta \theta_j = - \eta g_j$$
$$\Delta v_{ih} = \eta e_h x_i$$
$$\Delta \gamma_h = -\eta e_h$$

误差逆传播算法(BP算法)

---

输入：训练集$D={(\mathbf{x}, \mathbf{y})}_{k=1}^m$;
&emsp;学习率$\eta$

过程：
1. 在$(0,1)$范围内随机初始化网络中所有连接权和阈值
2. $repeat$
3. &emsp; $for all (\mathbf{x}_k, \mathbf{y}_k) \in D do$
4. &emsp;&emsp;根据当前参数和$\hat{y}_j^k=f(\beta_j-\theta_j)$计算当前样本的输出$\hat{\mathbf{y}}_k$
5. &emsp;&emsp;根据式$g_j=\hat{y}_j^k(1-\hat{y}_j^k)(y_j^k-\hat{y}_j^k)$计算输出层神经元的梯度项$g_j$
6. &emsp;&emsp;根据式$e_h=b_h(1-b_h)\sum_{j=1}^{l}\omega_{hj}g_j$计算隐层神经元的梯度项$e_h$
7. &emsp;&emsp;根据$\omega$等的更新公式来更新连接权$\omega_{hj},$v_{ih}$$与阈值$\theta_j$,$\gamma_h$
8. &emsp; $end for$
9. $until$ 达到停止条件

输出：连接权与阈值确定的多层前馈神经网络

需要注意的是，BP算法的目标是要最小化训练集$D$上的累计误差:

$$E = \frac{1}{m} \sum_{k=1}^{m} E_k$$


## 全局最小与局部最小

神经网络的训练过程可看作参数寻优过程，在参数空间中，寻找一组最优参数使得$E$最小。

在现实任务中，人们常采用多种策略来试图“跳出”局部极小，从而进一步接近全局最小：
1. 多组不同参数值初始化多个神经网络，按标准方法训练后，取其中误差最小的解作为最终参数
2. 模拟退火
3. 随机梯度下降
