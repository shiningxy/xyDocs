## 第二章 马尔可夫决策过程

### 关键词概念

- 马尔可夫性质(Markov Property): 如果某一个过程未来的转移跟过去是无关，只由现在的状态决定，那么其满足马尔可夫性质。换句话说，一个状态的下一个状态只取决于它当前状态，而跟它当前状态之前的状态都没有关系。
- 马尔可夫链(Markov Chain): 概率论和数理统计中具有马尔可夫性质（Markov property）且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）。
- 状态转移矩阵(State Transition Matrix): 状态转移矩阵类似于一个 conditional probability，当我们知道当前我们在$s_{t}$这个状态过后，到达下面所有状态的一个概念，它每一行其实描述了是从一个节点到达所有其它节点的概率。
- 马尔可夫奖励过程(Markov Reward Process, MRP)：即马尔可夫链再加上了一个奖励函数。在 MRP之中，转移矩阵跟它的这个状态都是跟马尔可夫链一样的，多了一个奖励函数(reward function)。奖励函数是一个期望，它说当你到达某一个状态的时候，可以获得多大的奖励。
- horizon: 定义了同一个 episode 或者是整个一个轨迹的长度，它是由有限个步数决定的。
- return: 把奖励进行折扣(discounted)，然后获得的对应的收益。
- Bellman Equation（贝尔曼等式）：定义了当前状态与未来状态的迭代关系，表示当前状态的值函数可以通过下个状态的值函数来计算。Bellman Equation 因其提出者、动态规划创始人 Richard Bellman 而得名 ，同时也被叫作“动态规划方程”。$V(s)=R(s)+\gamma{\textstyle \sum_{s'\in S}} P(s'|s)V(s')$，特别地，矩阵形式：$V=R+\gamma PV$。
- Monte Carlo Algorithm（蒙特卡洛方法）：可用来计算价值函数的值。通俗的将，我们当得到一个MRP过后，我们可以从某一个状态开始，然后让它把这个小船放进去，让它随波逐流，这样就会产生一个轨迹。产生了一个轨迹过后，就会得到一个奖励，那么就直接把它的Discounted的奖励$g$直接算出来。算出来过后就可以把它积累起来，当积累到一定的轨迹数量过后，然后直接除以这个轨迹，就会得到它的这个价值。
- Iterative Algorithm（动态规划方法）： 可用来计算价值函数的值。通过一直迭代对应的Bellman Equation，最后使其收敛。当这个最后更新的状态跟你上一个状态变化并不大的时候，这个更新就可以停止。
- Q函数 (action-value function)：其定义的是某一个状态某一个行为，对应的它有可能得到的 return 的一个期望（over policy function）。
- MDP中的prediction（即policy evaluation问题）： 给定一个 MDP 以及一个 policy$\pi$，去计算它的value function，即每个状态它的价值函数是多少。其可以通过动态规划方法（Iterative Algorithm）解决。
- MDP中的control问题：寻找一个最佳的一个策略，它的input就是MDP，输出是通过去寻找它的最佳策略，然后同时输出它的最佳价值函数（optimal value function）以及它的这个最佳策略（optimal policy）。optimal policy使得每个状态，它的状态函数都取得最大值。所以当我们说某一个MDP的环境被解了过后，就是说我们可以得到一个optimal value function，然后我们就说它被解了。

### 知识点总结

为什么在马尔可夫奖励过程（MRP）中需要有discount factor?
> 1. 首先，有些马尔可夫过程是**带环**的，它并没有终结，我们想要**避免无穷的奖励**；
> 2. 另外，我们是想把这个不确定性也表示出来，希望尽可能快地得到奖励，而不是在未来某一个点得到奖励；
> 3. 接上面一点，如果这个奖励它是有实际价值的了，我们可能是更希望立刻就得到奖励，而不是我们后面再得到奖励；
> 4. 还有在有些时候，这个系数也可以把它设为0。比如说，当我们设为0过后，然后我们就只关注了它当前的奖励。我们也可以把它设为1，设为1的话就是对未来并没有折扣，未来获得的奖励跟我们当前获得的奖励是一样的。
> 
> 所以，这个系数其实是应该可以作为强化学习agent的一个hyperparameter来进行调整，然后就会得到不同行为的agent。

为什么矩阵形式的Bellman Equation的解析解比较难解？
> 通过矩阵求逆的过程，就可以把这个V的这个价值的解析解直接求出来。但是一个问题是这个矩阵求逆的过程的复杂度是$O(N^3)$。所以就当我们状态非常多的时候，比如当我们有一百万个状态时，转移矩阵会是一个一百万乘以一百万的一个矩阵。这样一个大矩阵的求逆是非常困难的，所以这种通过解析解去解，只能用于很小量的MRP

计算贝尔曼等式（Bellman Equation）的常见方法以及区别？
> 1. Monte Carlo Algorithm （蒙特卡洛方法）：可用来计算价值函数的值。通俗的讲，我们当得到一个MRP过后，我们可以从某一个状态开始，然后让它把这个小船放进去，让它随波逐流，这样就会产生一个轨迹。产生了一个轨迹过后，就会得到一个奖励，那么就直接把它的Discounted的奖励$g$直接算出来。算出来过后就可以把它积累起来，当积累到一定的轨迹数量过后，然后直接除以这个轨迹，就会得到它的这个价值。
> 2. Iterative Algorithm（动态规划方2. ）： 可用来计算价值函数的值。通过一直迭代对应的Bellman Equation，最后使其收敛。当这个最后更新的状态跟你上一个状态变化并不大的时候，**通常是小于一个阈值$\gamma$**，这个更新就可以停止。
> 3. 以上两者的结合方法：另外我们也可以通过Temporal-Difference Learning的方法。也叫**TD Learning**，是动态规划和蒙特卡洛方法的一个结合

马尔可夫奖励过程（MRP）与马尔可夫决策过程（MDP）的区别？
> 相对于MRP，马尔可夫决策过程多了一个decision，其他的定义与MRP都类似。这里我们多了一个决策，多了一个action，那么这个状态转移也多了一个condition。即采取某一种行为，你未来的状态也会不同。它不仅是依赖于你当前的状态，也依赖于在当前状态你这个agent，agent采取的这个行为会决定它未来的这个状态走向。对于这个价值函数，它也是多了一个条件，多了一个你当前的这个行为，就是说你当前的状态以及你采取的行为会决定你当前可能得到多少奖励。
> 另外，两者之间是有转换关系的。具体来说，已知一个MDP以及一个policy$\pi$的时候，我们可以把MDP转换成MRP。在MDP里面，转移函数$P(s'|s,a)$是基于它当前状态以及它当前的action，因为我们现在已知它policy function，就是说在每一个状态，我们知道它可能采取的行为的概率

MDP里面的状态转移跟MRP以及MP的结构或者计算方面的差异？
> - **马尔可夫链**的转移是直接就决定。**从你当前是什么状态，直接通过转移概率就直接决定了你下一个状态会是什么。**
> - 但是对于**MDP，它中间多了一层行为action**。在当前这个状态，你首先要决定的是采取某一种行为。然后因为你有一定的不确定性，在当前状态决定你当前采取的行为过后，你到未来的状态其实也是一个概率分布。在此概率分布下多的这一层action，是说你有多大的概率到达某一个未来状态，或者说你有多大的概率到达另外一个状态。所以当前状态与未来状态的转移过程中多了一层决策性，这是MDP和之前的马尔可夫过程不同的一个地方。**多了一个component，agent会采取行为来决定未来的状态转移。**

我们如何寻找最佳的policy，方法有哪些？
> 本质来说，当我们取得最佳的价值函数过后，我们可以通过对这个Q函数进行极大化，然后得到最佳的价值。然后，我们直接在这个Q函数上取一个让这个action最大化的值，我们就可以直接提取出它的最佳的policy。
> 具体方法：
>   - 穷举法（一般不使用）：假设我们有有限多个状态、有限多个行为可能性，那么每个状态我们可以采取这个 A 种行为的策略，那么总共就是$|A|^{|S|}$个可能的policy。我们可以把这个穷举一遍，然后算出每种策略的value function，然后对比一下可以得到最佳策略。这种方法效率极低。
>  - **Policy iteration**：一种迭代方法，有两部分组成，下面两个步骤一直在迭代进行，最终收敛：（有些类似于ML中EM算法（期望-最大化算法））
>    - 第一个步骤是**policy evaluation**，即当前我们在优化这个policy$\pi$，所以在优化过程中得到一个最新的policy。
>    - 第二个步骤是**policy improvement**，即取得价值函数后，进一步推算出它的Q函数。得到Q函数过后，那我们就直接去取它的极大化。
>  - Value iteration：我们一直去迭代Bellman Optimality Equation，到了最后，它能逐渐趋向于最佳的策略，这是value iteration算法的精髓，就是我们去为了得到最佳的$v^*$，对于每个状态它的$v^*$这个值，我们直接把这个Bellman Optimality Equation进行迭代，迭代了很多次之后它就会收敛至最佳的policy以及其对应的状态，这里面是没有policy function的。

请问马尔可夫过程是什么?马尔可夫决策过程又是什么?其中马尔可夫最重要的性质是什么呢?
> 马尔可夫过程是一个二元组$<S,P>$，$S$为状态的集合，$P$为状态转移概率矩阵；而马尔可夫决策过程是一个五元组$<S,P,A,R,\gamma>$，其中R表示为从$S$到$S'$能够获得的奖励期望，$\gamma$为折扣因子，$A$为动作集合。
> **马尔可夫最重要的性质是下一个状态只与当前状态有关，与之前的状态无关，也就是$P[S_{t+1}|S_t]=P[S_{t+1}|S_1,S_2,...,S_t]$**

请问我们一般怎么求解马尔可夫决策过程？
> 我们直接求解马尔可夫决策过程可以**直接求解贝尔曼等式（动态规划方程）**，即$V(s)=R(s)+\gamma{\textstyle \sum_{s'\in S}} P(s'|s)V(s')$，特别地，矩阵形式：$V=R+\gamma PV$。但是贝尔曼等式很难求解且计算复杂度较高，所以可以使用动态规划，蒙特卡洛，时间差分等方法求解。

请问如果数据流不满足马尔可夫性怎么办？应该如何处理？
> 如果不满足马尔可夫性，即下一个状态与之前的状态也有关，若还仅仅用当前的状态来进行求解决策过程，势必导致决策的泛化能力变差。为了解决这个问题，可以**利用RNN对历史信息建模，获得包含历史信息的状态表征**。表征过程可以使用注意力机制等手段。最后在表征状态空间求解马尔可夫决策过程问题。

请分别写出基于状态值函数的贝尔曼方程以及基于动作值的贝尔曼方程。
> 基于状态值函数的贝尔曼方程
> $$v_\pi(s)= {\textstyle \sum_{a}^{}} π(a∣s){\textstyle \sum_{s',r}}p(s',r∣s,a)[r(s,a)+\gamma v_\pi(s')]$$
> 基于动作值的贝尔曼方程
> $$q_\pi(s,a)={\textstyle \sum_{s',r}}p(s',r∣s,a)[r(s',a)+\gamma v_\pi(s')]$$

请问最佳价值函数（optimal value function）$v^*$和最佳策略（optimal policy）$\pi^*$为什么等价呢？
> 最佳价值函数的定义为：$v^*(s)=max_\pi v^{\pi}(s)$即我们去搜索一种policy$\pi$来让每个状态的价值最大。$v^*$就是到达每一个状态，它的值的极大化情况。在这种极大化情况上面，我们得到的策略就可以说是它的最佳策略（optimal policy），即$\pi^*(s)=\argmax_av^{\pi}(s)$。最佳策略是的每个状态的价值函数都取得最大值。所以如果我们可以得到一个optimal value function，就可以说某一个MDP的环境被解。在这种情况下，它的最佳的价值函数是一致的，就它达到的这个上限的值是一致的，但这里可能有多个最佳的policy，就是说多个policy可以取得相同的最佳价值。

能不能手写一下第n不的值函数更新公式？另外，当n越来越大时，值函数的期望和方差分别变大还是变小？
> n越大，方差越大，期望偏差越小。值函数的更新公式：
> $$Q(S,A) \leftarrow Q(S,A)+\alpha[\sum_{i=1}^{n}\gamma^{i-1}R_{t+i}+\gamma^n\max_aQ(S',a)-Q(S,A)]$$
