# 第九章

后端非线性优化

官方教程文档: https://www.yuque.com/u1507140/vslam-hmh/swh0g4a0t452pwux

## 思考题


### 1. LM法与GN法与梯度下降法的区别与联系是什么？

区别：

* LM法：是一种混合方法，结合了梯度下降法和牛顿法的优点。它在每一步迭代中根据当前参数估计计算一个类似于牛顿法的更新，但引入了一个调整参数（λ）来控制步长的大小，以避免牛顿法可能出现的数值不稳定性。LM法通常用于非线性最小二乘问题，如拟合曲线或解决非线性方程组。
* GN法：是一种基于牛顿法的优化方法，用于解决非线性最小二乘问题。GN法在每一步迭代中使用了问题的雅可比矩阵（Jacobian）来近似计算海森矩阵（Hessian），然后用这个近似的海森矩阵来更新参数。GN法通常要求问题具有良好的初始猜测，以获得收敛性。
* 梯度下降法：是一种基于目标函数梯度的迭代优化方法。它根据当前位置的梯度方向来更新参数，以使目标函数值降低。梯度下降法通常适用于大规模数据和参数空间，并且不需要求解雅可比矩阵或海森矩阵。
联系：

* LM法和GN法都可以用于解决非线性最小二乘问题，它们都涉及到参数的迭代更新，而梯度下降法也可以用于类似的问题，尤其是在大规模问题中。
* GN法和LM法都使用了某种形式的参数更新规则，类似于梯度下降法中的参数更新。不同之处在于，GN法使用了更复杂的海森矩阵或其近似，而LM法引入了一个调整参数来平衡梯度下降和牛顿法的影响。

### 2. 矩阵分解如何求解最小二乘问题

矩阵分解可以用于求解最小二乘问题，尤其是在线性回归或其他拟合问题中。以下是一种使用矩阵分解来求解最小二乘问题的方法：

假设我们有一个线性模型：$Y = Xβ + ε$，其中 $Y$ 是观测到的响应变量，$X$ 是设计矩阵，$β$ 是待估计的参数向量，$ε$ 是误差向量。

普通最小二乘法（Ordinary Least Squares, OLS）：最简单的方法是使用普通最小二乘法，它的目标是最小化残差平方和，即最小化 $||Y - Xβ||^2$。

矩阵分解方法：将问题转化为求解β的矩阵方程。通过将上述模型转化为矩阵形式，可以得到：

$$Y = Xβ + ε$$

对上述方程两边同时左乘 $X$ 的伪逆（或广义逆，如果 $X$ 不是满秩矩阵），可以得到最小二乘估计的闭式解：

$$β = (X^T X)^(-1) X^T Y$$

这里，$(X^T X)^(-1)$ 是 X^T X 的逆矩阵的伪逆。这个方法是一种通过矩阵运算来求解最小二乘问题的有效方式。

奇异值分解（Singular Value Decomposition, SVD）：另一种方法是使用奇异值分解来分解设计矩阵 $X$。通过对 $X$ 进行奇异值分解，可以将 $X$ 分解为三个矩阵的乘积：$X = UΣV^T$，其中 $U$ 和 $V$ 是正交矩阵，$Σ$ 是对角矩阵，包含奇异值。然后可以使用这些分解来计算参数估计：

$$β = VΣ^(-1)U^T Y$$

这种方法对于处理大型矩阵和高维数据非常有用，因为它可以通过截断奇异值来降低维度。

