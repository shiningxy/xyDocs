## Task01 先导

**XAI(eXplainable Artificial Intelligence) 可解释人工智能**，力求解释深度学习的核心算法。基于深度学习人工智能的核心算法都源于黑盒模型，其生成结果在本质上是不可解释的，因此难以获得用户的信任。尤其是在风险大的应用场景，如医疗诊断、金融监管和自动驾驶。所以发展可解释人工智能(XAI)极为重要且紧迫。
基于深度学习AI 的不可解释性表现在诸多方面，有两种基本类型：

1. **原理上的不可解释性**。由于深度神经网络模型和算法通常十分复杂，加上黑盒学习的性质，AI通常无法对预测的结果给出自我解释，模型十分不透明，需要依靠第三方的解释系统或者人类专家的帮助才能看清其内部工作原理。我们可以先通过简单和直观的方式对神经网络进行事后解释，在一个神经网络结束训练后，通过各种方法从不同的角度对神经网络进行解释，尝试揭示其背后的决策机理，例如利用**可视化**、**神经网络输入单元重要性归因**等。
2. **语义上的不可解释性**。深度学习用于挖掘数据中变量之间的关联性，而数据关联性的产生机制有以下三种类型：因果、混淆和样本选择偏差。例如，一个基于深度学习神经网络的图像识别系统，把某个图像识别为狼，第一种：系统依据狼的【头部以及身体特征】，这种【解释】是具有稳定性和鲁棒性的。第二种：系统依据狼的某个【局部纹理】。第三种：系统依据【草原】而判断其为狼，后两种的结论可能是正确的，但这种依据混淆或样本选择偏差带来的虚假关联而做出的【解释】，一定是不稳定和缺乏鲁棒性的。遗憾的是，但深度学习神经网络算法通常找到的是虚假或表面的关联，而不是因果关系，这种【解释】对于研究者和开发者在内的解释受众来讲是不可接受的。要想解决这种类型的不可解释性，只能从改变深度学习模型做起。

可解释人工智能还涉及到生物医疗、金融、计算机视觉、自然语言处理及推荐系统。在高风险的应用领域对可解释人工智能提出了更高的要求。目前，以深度学习为主体的AI远没有达到可解释性的要求，因为我们这定义的【可解释性】，不仅要求模型对用户是透明的，能够解释其背后的工作原理；并且这种【解释】必须是本质的，具有稳定性和鲁棒性的。此外，如何将知识与深度学习模型相结合，或者导入因果关系，目前的工作都只是初步的尝试，还有待进一步深入。

---

在实际应用层面，通过刷海量数据的填鸭式学习得到的人工智能系统存在一系列隐患，并可能引发严重的社会问题，这造成了机器学习的如下三个应用缺陷：
1. 由于数据样本收集的局限和偏见，导致数据驱动的人工智能系统也是有偏见的，这种偏见甚至无异于人类社会中的偏见。
2. 黑盒似的深度神经网络还常常犯一些低级的，人类不可能犯的错误，表现出安全性上的潜在风险。比如对一张熊猫图片添加微小的噪声后，图片就被高置信度地认为是长臂猿。考虑到人脸识别在金融支付等场景的广泛应用，这种潜在的风险令人不寒而栗。
3. 从决策机制来看，当前对深度学习算法的分析还处于不透明的摸索阶段，尤其是拥有亿万个参数的超大规模预训练神经网络，如BERT、GPT3等，其决策过程在学术上没有清晰的说明。这种黑盒似的深度神经网络暂时无法获得人类的充分理解与信任。

---

### 可解释性AI行业应用中的不同类型解释
| **行业** | **面向开发者** | **面向监管者** | **面向使用者** | **面向应用用户** |
|----------|--------------------------------|-------------------------|---------------------------------------------|----------------------------------------|
| **生物医疗** | 人工智能归纳的信息和数据规律 | 符合伦理和法规要求模型的高可信度模型的高透明度 | 模型表征与医学知识的联系，可视化、语义化、关系代理 | 模型输出的合理性，可理解的诊断结果 |
| **金融** | 模型假设是否满足，模型逻辑是否自洽，模型代码是否正常符合预设 | 人工智能风险的可解释性与人工智能建模的可解释性 | 模型算法的决策可解释性、算法的可追溯性、算法对于数据使用的偏见预警、算法的风险可控制性 | 应用服务对象：算法决策依据、算法公平性，程序样本来源：隐私权保护、知情权保护 |
| **电商推荐** | 推荐算法内在的运作机制，专业的数字化解释  | 视觉上可解释的推荐模型、可解释产品搜索 | 可解释的序列推荐、跨类别的可解释性推荐 | 隐私权保护、知情权保护，基于特征的解释及用户评论 |
| **城市管理** | 推荐算法内在的运作机制，专业的数学化解释 | 模型的合理性和稳定性，数据的安全性 | 可解释的位置推荐、最有价值的特征的推荐和解释 | 不同地点之间的关系的解释、推荐位置的特征词云 |
| **安防**  | 安防算法内部的运行机制、行为逻辑和决策依据 | 模型的公平性、偏见性和稳定性 | 模型的安全性和可靠性 | 隐私权保护、知情权保护 |
| **法律咨询** | 咨询算法内部的运行机制，模型代码是否正常符合预设 | 可视化的知识图谱检索、推理和决策逻辑 | 基于知识引入的模型，可靠性 | 内容的即时性、针对性和准确性 |

在前期的的可解释AI相关研究中，比较有影响力的工作包括以下几类：
1. **基于专家知识规则的符号推理解释**：MYCIN被认为是最早的可解释专家系统之一，其首创的certainty-factor模型被用于处理专家系统中规则的**不确定性问题**，并对其后发展起来的**贝叶斯网络**有启迪作用。基于专家知识规则的解释具有逻辑性强、易于被人类使用者理解的有优点。但是，该方法有两个缺点：首先，基于规则的决策往往过于简单，不能灵活处理大千世界的多样性和不确定性，在决策性能上比数据驱动的机器学习方法有所欠缺；其次，专家知识规则的制定会耗费大量的人力及时间，因而更新困难。这就使得专家系统在建立后很难适应不断变化的场景需求，很快变得僵化、落伍。
2. **基于自动生成规则的推理机制解释**：20世纪90年代以来，研究人员试图将神经网络和规则推理两种方法的优点结合起来，取长补短。这些研究主要集中在以下三个问题：第一、如何从训练好的神经网络中自动提取规则，以补充到先验知识规则库中；第二，如何根据神经网络训练结果来修订知识库中的已有规则；第三，如何在神经网络的训练中引入先验的知识库及规则来约束神经网络的表现。早期的基本思想是利用逻辑推理规则对神经网络进行一定程度的简化和约束来提升其可解释性。另外，值得一提的是，为了从数值化的神经网络模型中提取对应的逻辑推理规则，早期的研究还提出了利用**模糊逻辑单元**来模拟神经元节点的输入和输出，这样的模糊处理方式能够让基于规则的巨册更加灵活地适应各种输入和输出信息的不确定性。同时，自动化的规则提取又使得整个知识规则库的更新变得灵活、高效。
3. **机器学习模型的可解释性**：有几类模型本身被认为是具有较强可解释性的，其中，**线性回归**模型将输出值表示为各输入特征的线性加权和，学习得到的特征权值除以特征权值的标准差，可以直接解释为该特征对输出影响的重要性。**逻辑回归**模型则是在线性回归的基础上，通过**logit函数**将模型输出映射为分类预测结果的概率。另一类常用的**决策树**模型，是通过将输入特征的高维空间进行连续切分来自动拟合样本数据。这样通过学习得到的特征及其切分值解释了相应的决策规则。而通过进一步增加决策树的深度，能够得到更加复杂的组合决策规则。