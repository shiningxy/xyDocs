> 正在梳理中，持续更新

## Task01 先导
**XAI(eXplainable Artificial Intelligence) 可解释人工智能**，力求解释深度学习的核心算法。基于深度学习人工智能的核心算法都源于黑盒模型，其生成结果在本质上是不可解释的，因此难以获得用户的信任。尤其是在风险大的应用场景，如医疗诊断、金融监管和自动驾驶。所以发展可解释人工智能(XAI)极为重要且紧迫。
基于深度学习AI 的不可解释性表现在诸多方面，有两种基本类型：

1. **原理上的不可解释性**。由于深度神经网络模型和算法通常十分复杂，加上黑盒学习的性质，AI通常无法对预测的结果给出自我解释，模型十分不透明，需要依靠第三方的解释系统或者人类专家的帮助才能看清其内部工作原理。我们可以先通过简单和直观的方式对神经网络进行事后解释，在一个神经网络结束训练后，通过各种方法从不同的角度对神经网络进行解释，尝试揭示其背后的决策机理，例如利用**可视化**、**神经网络输入单元重要性归因**等。
2. **语义上的不可解释性**。深度学习用于挖掘数据中变量之间的关联性，而数据关联性的产生机制有以下三种类型：因果、混淆和样本选择偏差。例如，一个基于深度学习神经网络的图像识别系统，把某个图像识别为狼，第一种：系统依据狼的【头部以及身体特征】，这种【解释】是具有稳定性和鲁棒性的。第二种：系统依据狼的某个【局部纹理】。第三种：系统依据【草原】而判断其为狼，后两种的结论可能是正确的，但这种依据混淆或样本选择偏差带来的虚假关联而做出的【解释】，一定是不稳定和缺乏鲁棒性的。遗憾的是，但深度学习神经网络算法通常找到的是虚假或表面的关联，而不是因果关系，这种【解释】对于研究者和开发者在内的解释受众来讲是不可接受的。要想解决这种类型的不可解释性，只能从改变深度学习模型做起。

可解释人工智能还涉及到生物医疗、金融、计算机视觉、自然语言处理及推荐系统。在高风险的应用领域对可解释人工智能提出了更高的要求。目前，以深度学习为主体的AI远没有达到可解释性的要求，因为我们这定义的【可解释性】，不仅要求模型对用户是透明的，能够解释其背后的工作原理；并且这种【解释】必须是本质的，具有稳定性和鲁棒性的。此外，如何将知识与深度学习模型相结合，或者导入因果关系，目前的工作都只是初步的尝试，还有待进一步深入。